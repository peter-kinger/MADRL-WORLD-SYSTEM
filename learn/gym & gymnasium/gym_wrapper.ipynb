{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "强化学习中会涉及很多的 wrapper 归一化处理过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation: [-0.01550082  0.24244374  0.02131401 -0.2551575 ], Reward: 0.1\n",
      "Observation: [-0.01065194  0.04702406  0.01621086  0.0441713 ], Reward: 0.1\n",
      "Observation: [-0.00971146 -0.14832655  0.01709429  0.3419245 ], Reward: 0.1\n",
      "Observation: [-0.01267799  0.04654808  0.02393278  0.05468074], Reward: 0.1\n",
      "Observation: [-0.01174703 -0.14890872  0.02502639  0.35481754], Reward: 0.1\n",
      "Observation: [-0.0147252  -0.3443774   0.03212274  0.6552857 ], Reward: 0.1\n",
      "Observation: [-0.02161275 -0.53993154  0.04522846  0.95790803], Reward: 0.1\n",
      "Observation: [-0.03241138 -0.34544593  0.06438661  0.67977065], Reward: 0.1\n",
      "Observation: [-0.0393203  -0.15127464  0.07798203  0.4080338 ], Reward: 0.1\n",
      "Observation: [-0.04234579 -0.34741068  0.0861427   0.7242473 ], Reward: 0.1\n",
      "Observation: [-0.04929401 -0.15357901  0.10062765  0.45987245], Reward: 0.1\n",
      "Observation: [-0.05236559  0.03998731  0.1098251   0.2005264 ], Reward: 0.1\n",
      "Observation: [-0.05156584  0.23338115  0.11383563 -0.05559294], Reward: 0.1\n",
      "Observation: [-0.04689822  0.03682661  0.11272377  0.27072608], Reward: 0.1\n",
      "Observation: [-0.04616169  0.23017463  0.11813829  0.01561535], Reward: 0.1\n",
      "Observation: [-0.04155819  0.42342162  0.1184506  -0.23758276], Reward: 0.1\n",
      "Observation: [-0.03308976  0.22682416  0.11369894  0.08999018], Reward: 0.1\n",
      "Observation: [-0.02855328  0.03027164  0.11549874  0.41627085], Reward: 0.1\n",
      "Observation: [-0.02794784 -0.16628157  0.12382416  0.7430175 ], Reward: 0.1\n",
      "Observation: [-0.03127348  0.02693353  0.13868451  0.4917252 ], Reward: 0.1\n",
      "Observation: [-0.03073481  0.21985479  0.14851902  0.24576443], Reward: 0.1\n",
      "Observation: [-0.02633771  0.4125779   0.1534343   0.00336646], Reward: 0.1\n",
      "Observation: [-0.01808615  0.6052045   0.15350163 -0.23724613], Reward: 0.1\n",
      "Observation: [-0.00598206  0.40826085  0.14875671  0.09964828], Reward: 0.1\n",
      "Observation: [0.00218316 0.21135475 0.15074968 0.43531886], Reward: 0.1\n",
      "Observation: [0.00641025 0.01445614 0.15945606 0.7714699 ], Reward: 0.1\n",
      "Observation: [0.00669937 0.2070667  0.17488545 0.532901  ], Reward: 0.1\n",
      "Observation: [0.01084071 0.00997228 0.18554348 0.87518764], Reward: 0.1\n",
      "Observation: [0.01104015 0.2021532  0.20304723 0.6461015 ], Reward: 0.1\n",
      "Observation: [0.01508322 0.39395398 0.21596925 0.4235972 ], Reward: 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\peter\\.conda\\envs\\torch_env\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from gym import spaces\n",
    "\n",
    "# 创建一个简单的环境\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "# 定义一个状态归一化的 Wrapper\n",
    "class NormalizeObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super(NormalizeObservation, self).__init__(env)\n",
    "        # 修改观察空间为归一化后的空间\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=env.observation_space.shape, dtype=np.float32)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        # 将原始观察值归一化到 [0, 1]\n",
    "        return (observation - self.observation_space.low) / (self.observation_space.high - self.observation_space.low)\n",
    "\n",
    "# 定义一个奖励缩放的 Wrapper\n",
    "class ScaleReward(gym.RewardWrapper):\n",
    "    def __init__(self, env, scale=0.1):\n",
    "        super(ScaleReward, self).__init__(env)\n",
    "        self.scale = scale\n",
    "\n",
    "    def reward(self, reward):\n",
    "        # 缩放奖励\n",
    "        return reward * self.scale\n",
    "\n",
    "# 使用 Wrappers\n",
    "wrapped_env = NormalizeObservation(env)\n",
    "wrapped_env = ScaleReward(wrapped_env)\n",
    "\n",
    "# 进行一次交互\n",
    "observation = wrapped_env.reset()\n",
    "for _ in range(1000):\n",
    "    action = wrapped_env.action_space.sample()  # 随机选择一个动作\n",
    "    state, reward, done, truncated, info = wrapped_env.step(action)\n",
    "    # state, reward, done, truncated, info = envs.step(action)\n",
    "    print(f\"Observation: {state}, Reward: {reward}\")\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "wrapped_env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
