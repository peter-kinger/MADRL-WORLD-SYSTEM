{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "教程来源：\n",
    "https://blog.csdn.net/chenxy_bwave/article/details/122617178"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gym 更新截止时间：\n",
    "\n",
    "截止2023年，Gym 已经不再更新或维护，最新版本为v0.26.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每个环境就代表着一类强化学习问题，用户通过设计和训练自己的智能体来解决这些强化学习问题。所以，某种意义上，Gym也可以看作是一个强化学习习题集！\n",
    "\n",
    "每个对应其中的习题库操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch_env 中的 gym 版本\n",
    "`gym  0.26.2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 44 envs in gym\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['CartPole-v0',\n",
       " 'CartPole-v1',\n",
       " 'MountainCar-v0',\n",
       " 'MountainCarContinuous-v0',\n",
       " 'Pendulum-v1',\n",
       " 'Acrobot-v1',\n",
       " 'LunarLander-v2',\n",
       " 'LunarLanderContinuous-v2',\n",
       " 'BipedalWalker-v3',\n",
       " 'BipedalWalkerHardcore-v3',\n",
       " 'CarRacing-v2',\n",
       " 'Blackjack-v1',\n",
       " 'FrozenLake-v1',\n",
       " 'FrozenLake8x8-v1',\n",
       " 'CliffWalking-v0',\n",
       " 'Taxi-v3',\n",
       " 'Reacher-v2',\n",
       " 'Reacher-v4',\n",
       " 'Pusher-v2',\n",
       " 'Pusher-v4',\n",
       " 'InvertedPendulum-v2',\n",
       " 'InvertedPendulum-v4',\n",
       " 'InvertedDoublePendulum-v2',\n",
       " 'InvertedDoublePendulum-v4',\n",
       " 'HalfCheetah-v2',\n",
       " 'HalfCheetah-v3',\n",
       " 'HalfCheetah-v4',\n",
       " 'Hopper-v2',\n",
       " 'Hopper-v3',\n",
       " 'Hopper-v4',\n",
       " 'Swimmer-v2',\n",
       " 'Swimmer-v3',\n",
       " 'Swimmer-v4',\n",
       " 'Walker2d-v2',\n",
       " 'Walker2d-v3',\n",
       " 'Walker2d-v4',\n",
       " 'Ant-v2',\n",
       " 'Ant-v3',\n",
       " 'Ant-v4',\n",
       " 'Humanoid-v2',\n",
       " 'Humanoid-v3',\n",
       " 'Humanoid-v4',\n",
       " 'HumanoidStandup-v2',\n",
       " 'HumanoidStandup-v4']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "from gym import envs\n",
    "env_list = envs.registry.keys()\n",
    "env_ids = [env_item for env_item in env_list]\n",
    "print(\"There are {0} envs in gym\".format(len(env_ids)))\n",
    "env_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "观测空间 =Discrete(2)\n",
      "动作空间 =Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)\n",
      "动作数 =2\n",
      "初始状态=(array([ 0.01413757,  0.00371064, -0.03867397, -0.02249677], dtype=float32), {})\n",
      "action=0\n",
      "初始状态=[ 0.01413757  0.00371064 -0.03867397 -0.02249677]\n"
     ]
    }
   ],
   "source": [
    "# 查看基本的环境创建打印部分\n",
    "# 后面 iseec 也是类似的基本操作\n",
    "env = gym.make('CartPole-v1')\n",
    "print(\"观测空间 ={}\".format(env.action_space))\n",
    "print(\"动作空间 ={}\".format(env.observation_space))\n",
    "print(\"动作数 ={}\".format(env.action_space.n))\n",
    "initial_state = env.reset()\n",
    "print(\"初始状态={}\".format(initial_state))\n",
    "# 环境执行一个动作\n",
    "action = env.action_space.sample()\n",
    "print(\"action={}\".format(action))\n",
    "print(\"初始状态={}\".format(env.state))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 几个基本组件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在最开始的任务里面需要调用 reset()\n",
    "\n",
    "> 注意每次迭代更新参数都会变化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## env.state\n",
    "\n",
    "注意变量和函数的调用括号"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## env.step() 单步执行操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "state, reward, terminated,truncated, info = env.step(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意每次在里面对应只操作 agent 与 env 的交互部分\n",
    "\n",
    "执行一个动作更新一个 state，实时反馈其中的对应 reward\n",
    "\n",
    "返回值返回“下一个状态”（object）、“报酬”（float）、“ episode 是否完成”（bool）、“日志信息”（dict）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "动作 = 1: 当前状态 = [-0.00460494 -0.18750563 -0.01805273  0.1840865 ], 奖励 = 1.0, 结束标志 = False, 日志信息 = {}\n",
      "动作 = 0: 当前状态 = [-0.00835506 -0.3823647  -0.014371    0.47102025], 奖励 = 1.0, 结束标志 = False, 日志信息 = {}\n",
      "动作 = 0: 当前状态 = [-0.01600235 -0.5772807  -0.00495059  0.7591392 ], 奖励 = 1.0, 结束标志 = False, 日志信息 = {}\n",
      "动作 = 0: 当前状态 = [-0.02754797 -0.7723341   0.01023219  1.0502602 ], 奖励 = 1.0, 结束标志 = False, 日志信息 = {}\n",
      "动作 = 1: 当前状态 = [-0.04299465 -0.57734936  0.0312374   0.7608066 ], 奖励 = 1.0, 结束标志 = False, 日志信息 = {}\n"
     ]
    }
   ],
   "source": [
    "# 单次执行奖励操作部分\n",
    "for k in range(5):\n",
    "    action = env.action_space.sample()\n",
    "    # 参数说明\n",
    "    # state: 状态\n",
    "    # reward: 奖励\n",
    "    # done: 是否结束\n",
    "    # truncated: 是否截断, 一般用于判断是否终止\n",
    "    # （通常是时间截断，也可以是物理意义上的边界截止）\n",
    "    # 用来非正常结束状态时强制结束仿真\n",
    "    # info: 信息\n",
    "    state, reward, done, truncated, info = env.step(action)\n",
    "    print('动作 = {0}: 当前状态 = {1}, 奖励 = {2}, 结束标志 = {3}, 日志信息 = {4}'.format(action, state, reward, done, info))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## env.render() 环境显示"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "渲染模式中支持好多种操作对其进行显示：\n",
    "比如：\n",
    "- human\n",
    "- rgb_array\n",
    "- ansi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## env.close() 关闭环境"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## env.sample_space.sample():"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "动作应该是基于策略来进行，\n",
    "随机粗略也叫作基线策略：为什么呢，因为如果简单的基线策略都没有超过策略，证明策略确实很烂，比如随机采取的都没有超过\n",
    "\n",
    "在股票预测中，如果我们随机选择买入或卖出，那么这就是基线策略。任何一个有效的预测算法都应该能在此基础上表现更好。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## env.seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可重复性: 如果你在不同的实验中多次运行这段代码，使用相同的种子（42），每次实验中的初始状态、选择的动作和下一个状态将始终相同。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有三种参数展示操作，考虑可视化放在这里实现\n",
    "\n",
    "报错解决：\n",
    "https://blog.csdn.net/lyx369639/article/details/127005933"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 随机策略的完整示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.26.2\n"
     ]
    }
   ],
   "source": [
    "print(gym.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state [-0.03218992 -0.21944998 -0.01656516  0.2657331 ], reward 1.0\n",
      "state [-0.03657892 -0.02409557 -0.0112505  -0.03212824], reward 1.0\n",
      "state [-0.03706083  0.1711859  -0.01189306 -0.3283395 ], reward 1.0\n",
      "state [-0.03363712 -0.02376475 -0.01845985 -0.03943069], reward 1.0\n",
      "state [-0.03411241  0.17161699 -0.01924847 -0.33788025], reward 1.0\n",
      "state [-0.03068007  0.3670075  -0.02600607 -0.6365704 ], reward 1.0\n",
      "state [-0.02333992  0.17225768 -0.03873748 -0.3521894 ], reward 1.0\n",
      "state [-0.01989477 -0.02229261 -0.04578127 -0.07196885], reward 1.0\n",
      "state [-0.02034062  0.17345476 -0.04722064 -0.37873724], reward 1.0\n",
      "state [-0.01687153  0.36921442 -0.05479539 -0.6859271 ], reward 1.0\n",
      "state [-0.00948724  0.17489423 -0.06851393 -0.41098613], reward 1.0\n",
      "state [-0.00598935  0.3709172  -0.07673365 -0.72445804], reward 1.0\n",
      "state [ 0.00142899  0.5670117  -0.09122282 -1.0402719 ], reward 1.0\n",
      "state [ 0.01276923  0.7632194  -0.11202825 -1.360142  ], reward 1.0\n",
      "state [ 0.02803361  0.95955324 -0.13923109 -1.6856642 ], reward 1.0\n",
      "state [ 0.04722468  0.76629    -0.17294437 -1.4393764 ], reward 1.0\n",
      "state [ 0.06255048  0.5736687  -0.2017319  -1.2053472 ], reward 1.0\n",
      "state [ 0.07402385  0.770743   -0.22583884 -1.5538716 ], reward 1.0\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import time \n",
    "import pygame # 需要按照这么库操作\n",
    "\n",
    "# 生成环境\n",
    "env = gym.make('CartPole-v1', render_mode='human')\n",
    "# 初始化环境\n",
    "state = env.reset()\n",
    "# 循环交互操作\n",
    "while True:\n",
    "    # 渲染画面\n",
    "    env.render()\n",
    "    # 从动作空间来随机hub选一个动作\n",
    "    action = env.action_space.sample() # 类似于 agent 实现的部分操作为 agent 的实现\n",
    "    ########################\n",
    "    # 隐式的 agent 操作交互\n",
    "    ########################\n",
    "    # 新版里面读的东西不同了\n",
    "    # 旧版代码： state, reward, done, _ = env.step(action)  # 更新为返回4个值 \n",
    "    next_state, reward, done, truncated, info = env.step(action) # 更新为返回5个值 # 怪不得读不懂，通常后面都省略了这部分\n",
    "    print(\"state {0}, reward {1}\".format(next_state, reward))\n",
    "    # 判断是否结束了\n",
    "    if done:\n",
    "        print(\"done\")\n",
    "        break\n",
    "    # 增加时间间隔，看清内容\n",
    "    time.sleep(0.4)\n",
    "    \n",
    "# 环境结束\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
